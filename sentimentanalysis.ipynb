{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lemrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lemrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lemrk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vectorized data: (10261, 10000)\n",
      "Best Parameters for Logistic Regression: {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Logistic Regression Accuracy: 0.8904910366328916\n",
      "XGBoost Accuracy: 0.886983632112237\n",
      "Random Forest Accuracy: 0.8819173811379579\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.59      0.19      0.29       120\n",
      "     Neutral       0.48      0.21      0.29       189\n",
      "    Positive       0.91      0.98      0.95      2257\n",
      "\n",
      "    accuracy                           0.89      2566\n",
      "   macro avg       0.66      0.46      0.51      2566\n",
      "weighted avg       0.86      0.89      0.87      2566\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[  23   16   81]\n",
      " [   8   40  141]\n",
      " [   8   27 2222]]\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer  # Using Lemmatizer instead of Porter Stemmer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')  # Ensure punkt is downloaded\n",
    "nltk.download('stopwords')  # Ensure stopwords are downloaded\n",
    "nltk.download('wordnet')  # Ensure WordNet is downloaded for lemmatization\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = r\"C:\\Users\\lemrk\\Music\\Sentiment-Analysis\\Instruments_Reviews.csv\"\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "dataset[\"reviewText\"] = dataset[\"reviewText\"].fillna(value=\"\")  # Fill missing reviewText\n",
    "dataset[\"reviews\"] = dataset[\"reviewText\"] + \" \" + dataset[\"summary\"]  # Combine reviewText and summary\n",
    "dataset = dataset.drop(columns=[\"reviewText\", \"summary\"], axis=1)  # Drop unnecessary columns\n",
    "\n",
    "# Labelling Products Based On Ratings Given\n",
    "def Labelling(Rows):\n",
    "    if Rows[\"overall\"] > 3.0:\n",
    "        Label = \"Positive\"\n",
    "    elif Rows[\"overall\"] < 3.0:\n",
    "        Label = \"Negative\"\n",
    "    else:\n",
    "        Label = \"Neutral\"\n",
    "    return Label\n",
    "\n",
    "dataset[\"sentiment\"] = dataset.apply(Labelling, axis=1)\n",
    "\n",
    "# Text Preprocessing\n",
    "def Text_Cleaning(Text):\n",
    "    Text = Text.lower()  # Convert to lowercase\n",
    "    punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))  # Remove punctuation\n",
    "    Text = Text.translate(punc)\n",
    "    Text = re.sub(r'\\d+', '', Text)  # Remove numbers\n",
    "    Text = re.sub(r'https?://\\S+|www\\.\\S+', '', Text)  # Remove URLs\n",
    "    Text = re.sub('\\n', '', Text)  # Remove newlines\n",
    "    return Text\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords\n",
    "Stopwords = set(nltk.corpus.stopwords.words(\"english\")) - set([\"not\", \"no\", \"nor\"])  # Keep negation words\n",
    "Stopwords = list(Stopwords)  # Convert set to list for CountVectorizer\n",
    "\n",
    "def Text_Processing(Text):\n",
    "    Processed_Text = list()\n",
    "    Tokens = nltk.word_tokenize(Text)  # Tokenize the text\n",
    "    for word in Tokens:\n",
    "        if word not in Stopwords:\n",
    "            Processed_Text.append(lemmatizer.lemmatize(word))  # Use Lemmatizer\n",
    "    return \" \".join(Processed_Text)\n",
    "\n",
    "# Apply text cleaning and processing\n",
    "dataset[\"reviews\"] = dataset[\"reviews\"].apply(lambda Text: Text_Cleaning(Text))\n",
    "dataset[\"reviews\"] = dataset[\"reviews\"].apply(lambda Text: Text_Processing(Text))\n",
    "\n",
    "# Feature Engineering\n",
    "Columns = [\"reviewerID\", \"overall\"]\n",
    "dataset = dataset.drop(columns=Columns, axis=1)  # Drop unnecessary columns\n",
    "\n",
    "# Encoding Our Target Variable\n",
    "Encoder = LabelEncoder()\n",
    "dataset[\"sentiment\"] = Encoder.fit_transform(dataset[\"sentiment\"])\n",
    "\n",
    "# TF-IDF Vectorizer with Better Parameters\n",
    "TF_IDF = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), stop_words=Stopwords)\n",
    "X = TF_IDF.fit_transform(dataset[\"reviews\"])\n",
    "y = dataset[\"sentiment\"]\n",
    "\n",
    "# Print the shape of the vectorized data\n",
    "print(f\"Shape of vectorized data: {X.shape}\")\n",
    "\n",
    "# Splitting Our Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_search.best_params_)\n",
    "\n",
    "# Train Logistic Regression with Best Parameters\n",
    "best_logreg = grid_search.best_estimator_\n",
    "y_pred_logreg = best_logreg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "\n",
    "# Train XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Train Random Forest\n",
    "rforest = RandomForestClassifier()\n",
    "rforest.fit(X_train, y_train)\n",
    "y_pred_rforest = rforest.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rforest))\n",
    "\n",
    "# Classification Report for Logistic Regression\n",
    "print(\"\\nClassification Report for Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_logreg, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Save the Logistic Regression model\n",
    "with open('logreg_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_logreg, f)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(TF_IDF, f)\n",
    "\n",
    "# Save the Label Encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(Encoder, f)\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
